{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJOdMcKTsASk"
      },
      "outputs": [],
      "source": [
        "import tensorflow as ts\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the data into train and test sets"
      ],
      "metadata": {
        "id": "83CfqbNOtFr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "ZHRKH--bsmIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The data is 28x28 pixel handwriting digits images (0 - 9)"
      ],
      "metadata": {
        "id": "Nfz0pCGluNtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'X Train shape: {x_train.shape} \\n ')\n",
        "\n",
        "print(f'X Test shape: {x_test.shape} \\n ')\n"
      ],
      "metadata": {
        "id": "igr2gleuta9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flattening  the image to a 1x784 vector"
      ],
      "metadata": {
        "id": "ZzQG2WtEu4FL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_vector_size = 28*28\n",
        "x_train = x_train.reshape(x_train.shape[0], image_vector_size)\n",
        "x_test = x_test.reshape(x_test.shape[0], image_vector_size)"
      ],
      "metadata": {
        "id": "K9KUd6fZujia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Making the lables from numrical to a categorical ones (good for cost functuon and tranig)"
      ],
      "metadata": {
        "id": "vUjgvTb4vhiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "num_classes = 10\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "bXxsPpekwyEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikeras "
      ],
      "metadata": {
        "id": "mu4dLqC0dfYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential # Basic model wich on top of we'll add layares \n",
        "from tensorflow.keras.layers import Dense,BatchNormalization,Dropout # A danse layer obcjet that we add to Sequential model\n",
        "from scikeras.wrappers import KerasClassifier "
      ],
      "metadata": {
        "id": "bYZv34VJeC_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Building a model and tuning it's hyperparameters as folow:\n",
        "####1. Build a genral model functuon \"model_generator\" \n",
        "####2. Add layer/s with pramas as var with the \".add()\" method\n",
        "####3. Build a GridSearchCV/Random object and pass it the params\n",
        "####4. Fit the data(traind) with .fit() method.\n",
        "####5. Check the scores "
      ],
      "metadata": {
        "id": "b7r6WBsRNaIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import itertools\n",
        "import time\n",
        "from keras.callbacks import EarlyStopping\n",
        "def model_generator(neurons,activation,optimizer,dropout_rate,norm=False,regularization=False):\n",
        "  model = Sequential()\n",
        "  if norm:\n",
        "    if regularization:\n",
        "      for i in range(len(neurons)):\n",
        "        model.add(Dense(neurons[i],activation=activation[0]))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    else:\n",
        "        for i in range(len(neurons)):\n",
        "          model.add(Dense(neurons[i],activation=activation[0]))\n",
        "          model.add(BatchNormalization())\n",
        "  elif regularization:\n",
        "        for i in range(len(neurons)):\n",
        "          model.add(Dense(neurons[i],activation=activation[0]))\n",
        "          model.add(Dropout(dropout_rate))\n",
        "  else:\n",
        "    for i in range(len(neurons)):\n",
        " \n",
        "      model.add(Dense(neurons[i],activation=activation[0]))\n",
        "\n",
        "  model.add(Dense(10,activation=activation[1]))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def MyRandomSearchCV(neurons: list,activation:list,optimizer:list,norm=False,regularization=False,epochs=30,batch_size=100,dropout_rate=0.2,early_stop=False,model=False):\n",
        "    random_indexs_neutrons = tuple(set([random.randint(0,len(neurons) -1 ) for i in range(int(len(neurons) * 0.1 + 1))]))\n",
        "    random_indexs_activation = tuple(set([random.randint(0,len(activation) - 1) for i in range(int(len(activation) * 0.1 + 1))]))\n",
        "    random_indexs_optimizer = tuple(set([random.randint(0,len(optimizer) - 1) for i in range(int(len(optimizer) * 0.1 + 1))]))\n",
        "    list_of_list = [random_indexs_neutrons,random_indexs_activation,random_indexs_optimizer]\n",
        "    all_combintation = tuple((itertools.product(*list_of_list)))\n",
        "    accuracy_map = {}\n",
        "    model_map = {}\n",
        "    for combination in all_combintation:\n",
        "    \n",
        "      neurons_index,activation_index,optimizer_index = combination\n",
        "      model = model_generator(neurons=neurons[neurons_index],\n",
        "                              activation=activation[activation_index],optimizer=optimizer[optimizer_index],\n",
        "                              norm=norm,regularization=regularization,dropout_rate=dropout_rate)\n",
        "      if early_stop:\n",
        "        es = EarlyStopping(monitor='val_loss', mode='min',patience=1, verbose=1)\n",
        "        model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=.1, verbose=True,callbacks=[es])\n",
        "      else:\n",
        "        model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=.1, verbose=False)\n",
        "      loss, accuracy  = model.evaluate(x_test, y_test, verbose=False)\n",
        "      key = (tuple(neurons[neurons_index]),tuple(activation[activation_index]),optimizer[optimizer_index])\n",
        "      model_map[key] = {'accuracy' : accuracy,'loss' : loss, 'model':model}\n",
        "      \n",
        "    return model_map\n"
      ],
      "metadata": {
        "id": "8CKrna1kUYkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adam v.s SGD"
      ],
      "metadata": {
        "id": "YGW0TKoy-xT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "T = time.time()\n",
        "adam_ac = 0\n",
        "num_of_iter = 1 \n",
        "neurons = [(60,30,15)]\n",
        "activations = [('relu','sigmoid')]\n",
        "optimizer_adam = ['adam']\n",
        "optimizer_sgd = ['sgd']\n",
        "\n",
        "for i in range(num_of_iter):\n",
        "  adam_ac += MyRandomSearchCV(neurons,activations,optimizer_adam)[(neurons[0],activations[0],optimizer_adam[0])]['accuracy']\n",
        "print(f'Adam\\'s AVG traning time is {(time.time() - T)/num_of_iter}\\n')\n",
        "print(f'Adam\\'s AVG accuracy is {adam_ac/num_of_iter}\\n')\n",
        "T = time.time()\n",
        "sgd_ac = 0\n",
        "for i in range(num_of_iter):\n",
        "  sgd_ac +=  MyRandomSearchCV(neurons,activations,optimizer_sgd)[(neurons[0],activations[0],optimizer_sgd[0])]['accuracy']\n",
        "print(f'SGD\\'sAVG traning time is {(time.time() - T)/num_of_iter}\\n')\n",
        "print(f'SGD\\'s AVG accuracy is {sgd_ac/num_of_iter}\\n')"
      ],
      "metadata": {
        "id": "WkBXLtY--sZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now let's use batch normalization"
      ],
      "metadata": {
        "id": "cT37wieMCuA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = time.time()\n",
        "norm_ac = 0\n",
        "num_of_iter = 1\n",
        "for i in range(num_of_iter):\n",
        "  norm_ac += MyRandomSearchCV(neurons,activations,optimizer_adam ,norm=True)[(neurons[0],activations[0],optimizer_adam[0])]['accuracy']\n",
        "print(f'Adam\\'s AVG traning time is {(time.time() - T)/num_of_iter}\\n')\n",
        "print(f'Adam\\'s AVG accuracy is {(norm_ac)/num_of_iter}\\n')\n",
        "\n"
      ],
      "metadata": {
        "id": "F4YWb2RmBhWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now let's Add dropout regularization with norm "
      ],
      "metadata": {
        "id": "sVolQqByeM-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = time.time()\n",
        "reg_ac = 0\n",
        "num_of_iter = 1\n",
        "for i in range(num_of_iter):\n",
        "  reg_ac += MyRandomSearchCV(neurons,activations,optimizer_adam,norm=True,regularization=True)[(neurons[0],activations[0],optimizer_adam[0])]['accuracy']\n",
        "print(f'Adam\\'s AVG traning time is {(time.time() - T)/num_of_iter}\\n')\n",
        "print(f'Adam\\'s AVG accuracy is {(reg_ac)/num_of_iter}\\n')"
      ],
      "metadata": {
        "id": "pV9ewxw8eMdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Only regularization"
      ],
      "metadata": {
        "id": "hokN0KiFipx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = time.time()\n",
        "reg_ac1 = 0\n",
        "num_of_iter = 1\n",
        "neurons = [(90,60,30,15)]\n",
        "for i in range(num_of_iter):\n",
        "  reg_ac1 += MyRandomSearchCV(neurons,activations,optimizer_adam,regularization=True,dropout_rate=0.25)[(neurons[0],activations[0],optimizer_adam[0])]['accuracy']\n",
        "print(f'Adam\\'s AVG traning time is {(time.time() - T)/num_of_iter}\\n')\n",
        "print(f'Adam\\'s AVG accuracy is {(reg_ac1)/num_of_iter}\\n')"
      ],
      "metadata": {
        "id": "ticBbaB3iXfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Early stopage\n",
        "####Want to see when the acuurcy is not changing significantly and stop"
      ],
      "metadata": {
        "id": "-Mw98BjFvqMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "T = time.time()\n",
        "num_of_iter = 1\n",
        "accuracy = 0\n",
        "neurons = [(60,30,15)]\n",
        "for i in range(num_of_iter):\n",
        " res = MyRandomSearchCV(neurons,activations,optimizer_adam,early_stop=True)[(neurons[0],activations[0],optimizer_adam[0])]\n",
        " accuracy += res['accuracy']\n",
        " model = res['model']\n",
        "print(f\"Accuracy = {accuracy/num_of_iter}\")\n",
        "print(f\"Time = {time.time() - T}\")"
      ],
      "metadata": {
        "id": "uvlRfGYyvnf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We'll define two functions that will give us more metrics and visualize it"
      ],
      "metadata": {
        "id": "kQIfHzlYGO_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "def get_categories(model,y_test):\n",
        "\n",
        "  pred = model.predict(x_test)\n",
        "  y_hat = []\n",
        "  y = []\n",
        "  for i in range(len(pred)):\n",
        "    y_hat.append(list(pred[i]).index(max(pred[i])))\n",
        "    y.append(list(y_test[i]).index(max(y_test[i])))\n",
        "  return y, y_hat\n",
        "\n",
        "def conf_matrix(model,y_test,font=0.8,fig=(10,10)):\n",
        "  y, y_hat = get_categories(model,y_test)\n",
        "  cm=confusion_matrix(y,y_hat)\n",
        "  report = classification_report(y,y_hat) \n",
        "  df_cm = pd.DataFrame(cm, index = [i for i in \"0123456789\"],\n",
        "                  columns = [i for i in \"0123456789\"])\n",
        "  plt.figure(figsize = fig)\n",
        "  sns.set(font_scale=font)\n",
        "  hm = sns.heatmap(df_cm, annot=True)\n",
        "  print(hm)\n",
        "  print(report)\n",
        "  return y,y_hat\n",
        "\n"
      ],
      "metadata": {
        "id": "NVpr6cgYwO4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y,y_hat=conf_matrix(model=model,y_test=y_test)"
      ],
      "metadata": {
        "id": "rkMlG7GdFOgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's see the actual pics the model got wrong"
      ],
      "metadata": {
        "id": "o0hahHWAGndr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "def get_pics(test_val,pred_val,num_of_pics,y,y_hat):\n",
        "  (x_train2, y_train2), (x_test2, y_test2) = mnist.load_data()\n",
        "  data = [x_test2[i] for i in range(len(y)) if y[i] == test_val and y_hat[i] == pred_val]\n",
        "  for d in range(num_of_pics):\n",
        "    plt.imshow(data[d], interpolation='nearest')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ZCqyE8qT_lUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_pics(7,1,10,y,y_hat)"
      ],
      "metadata": {
        "id": "TDSQJxsZGDBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fix it with more layers and neurons\n",
        "### Note: it did better on some and worst on other"
      ],
      "metadata": {
        "id": "s3GMaCNAJkZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neurons = [(90,60,45,30,15)]\n",
        "activations = [('relu','sigmoid')]\n",
        "optimizer_adam = ['adam']\n",
        "res = MyRandomSearchCV(neurons,activations,optimizer_adam,early_stop=True)[(neurons[0],activations[0],optimizer_adam[0])]\n",
        "accuracy = res['accuracy']\n",
        "model = res['model']\n",
        "conf_matrix(model,y_test)\n",
        "print(f\"Time :\\n {time.time() - T}\")"
      ],
      "metadata": {
        "id": "JyIEx48YEuM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traying the softmax activation function inested of sigmoid"
      ],
      "metadata": {
        "id": "JXVIb3KwIYrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neurons=[(45,30,15)]\n",
        "activations = [('relu','softmax')]\n",
        "res = MyRandomSearchCV(neurons,activations,optimizer_adam,early_stop=True)[(neurons[0],activations[0],optimizer_adam[0])]\n",
        "accuracy = res['accuracy']\n",
        "model = res['model']\n",
        "conf_matrix(model,y_test)\n",
        "print(f\"Time :\\n {time.time() - T}\")"
      ],
      "metadata": {
        "id": "9Kc7c0TsGiiN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}